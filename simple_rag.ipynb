{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c515c6b6",
   "metadata": {},
   "source": [
    "# Simple financial analysis RAG\n",
    "This notebook builds a lightweight Retrieval-Augmented Generation (RAG) pipeline using LangChain and an in-memory Qdrant vector store.\n",
    "\n",
    "It’s designed to demonstrate how to:\n",
    "1. Embed content from PDF documents.\n",
    "2. Split it into manageable chunks for effective semantic search.\n",
    "3. Store those chunks in a fast vector database (Qdrant) for retrieval.\n",
    "4. Answer a user question by retrieving relevant chunks and prompting a language model.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./docs/simple_rag.png\" alt=\"RAG pipeline\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4414c0",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758a028",
   "metadata": {},
   "source": [
    "### Set up environment variables for LangSmith and OpenAI\n",
    "\n",
    "This cell configures your environment with the necessary API keys for LangSmith and OpenAI:\n",
    "- ```LANGSMITH_TRACING``` is enabled to track and visualize LangChain executions.\n",
    "- ```LANGSMITH_API_KEY``` is securely requested via getpass and stored as an environment variable.\n",
    "- If ```OPENAI_API_KEY``` is not already defined in the environment, it prompts the user to enter it securely.\n",
    "\n",
    "Using getpass ensures that sensitive credentials are not exposed in plaintext in the notebook or logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f514e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4837bc",
   "metadata": {},
   "source": [
    "### Initialize OpenAI LLM\n",
    "We use GPT-4o-mini as the LLM — a fast, low-cost model well suited for focused, retrieval-enhanced tasks like this. It’s ideal when paired with a retriever because the LLM doesn’t need to “know everything” — it just needs to reason well with the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772574d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af470858",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Embeddings\n",
    "This cell sets up the embedding model used to convert text into numerical vector representations.\n",
    "- It uses text-embedding-3-small, a lightweight model from OpenAI optimized for speed and cost.\n",
    "- These embeddings are used to index and semantically retrieve relevant chunks of documents.\n",
    "- The OpenAIEmbeddings class provides a simple wrapper for calling the embedding API behind the scenes.\n",
    "\n",
    "This is a crucial component in a RAG pipeline, as it determines how well similar pieces of information are matched during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb087997",
   "metadata": {},
   "source": [
    "### Set up Qdrant vector store (local, persistent)\n",
    "\n",
    "This cell initializes a local Qdrant vector store to persist and manage document embeddings:\n",
    "- QdrantClient(path=\"qdrant_data\") creates (or reuses) a local database at the specified path. This enables reuse across notebook runs or from external APIs.\n",
    "- A collection named \"fin-docs\" is created to store vectors with:\n",
    "- Vector_size = 1536, which matches OpenAI’s text-embedding-3-small model dimensionality.\n",
    "- Distance.COSINE as the similarity metric, ideal for measuring semantic closeness in high-dimensional spaces.\n",
    "\n",
    "This setup enables fast and efficient semantic search over embedded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd7794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "vector_size = 1536 # https://www.pinecone.io/learn/openai-embeddings-v3/\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Create a collection with the specified vector size and distance metric\n",
    "client.create_collection(\n",
    "    collection_name=\"fin-docs\",\n",
    "    vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dc104",
   "metadata": {},
   "source": [
    "## Connect LangChain to Qdrant \n",
    "This cell links the Qdrant vector store with LangChain, allowing it to store and retrieve document embeddings using the specified collection and embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66290099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"fin-docs\",\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df822fb",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "### Asynchronously load PDF documents from a folder\n",
    "We define and run a function to load all PDF files found in the data/ directory using LangChain’s PyPDFLoader.\n",
    "- Each PDF is processed page by page, and every page is returned as a Document object.\n",
    "- The use of ``alazy_load()`` enables asynchronous loading, which improves efficiency when dealing with multiple or large files.\n",
    "- The result is a list of pages (``all_pages``) that will later be split, embedded, and indexed.\n",
    "\n",
    "This step prepares the raw document data for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c835759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "async def load_all_pdfs_from_folder(folder_path: str):\n",
    "    pages = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            async for page in loader.alazy_load():\n",
    "                pages.append(page)\n",
    "    return pages\n",
    "\n",
    "all_pages = await load_all_pdfs_from_folder(\"data\")\n",
    "\n",
    "print(f\"Loaded {len(all_pages)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e0c42",
   "metadata": {},
   "source": [
    "Let's check the object types and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_pages), type(all_pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_pages[20].page_content[4000:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a115e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total characters: {len(all_pages[20].page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd836d",
   "metadata": {},
   "source": [
    "### Split long pages into smaller, overlapping chunks\n",
    "\n",
    "Each page in the loaded PDFs contains approximately 5,000 characters, which is too large to send directly to the language model in a prompt. To handle this, we split the pages into smaller, overlapping text chunks using RecursiveCharacterTextSplitter:\n",
    "- Each chunk is up to 1,000 characters long with a 200-character overlap, preserving context across chunk boundaries.\n",
    "- ``add_start_index=True`` tracks the position of each chunk within the original document for potential traceability or highlighting.\n",
    "\n",
    "This chunking strategy is essential for a RAG pipeline, allowing efficient semantic retrieval and ensuring the model receives focused, context-rich inputs without exceeding token limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(all_pages)\n",
    "\n",
    "print(f\"Split guidebook data into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b34112",
   "metadata": {},
   "source": [
    "Store all the previously split document chunks in the Qdrant vector store. Each chunk is assigned a unique identifier, which is returned as a list. These IDs can be used later for referencing, tracing, or debugging. Indexing the content in this way enables efficient semantic retrieval when answering queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f393c5",
   "metadata": {},
   "source": [
    "### Load and inspect a RAG prompt template from LangChain Hub\n",
    "\n",
    "The code pulls a **ready-to-use RAG prompt** (rlm/rag-prompt) from the LangChain Hub. This prompt is designed to take in a retrieved context and a user question, format them appropriately, and prepare them for input to a language model.\n",
    "\n",
    "Using hub.pull simplifies experimentation by providing a standardized prompt format, but:\n",
    "- You can **fully customize this prompt** to match your domain, tone, or structure.\n",
    "- Custom prompts are especially useful when targeting specific use cases or needing tighter control over model behavior.\n",
    "\n",
    "To validate the structure, the prompt is invoked with dummy inputs and the resulting formatted message is printed. This helps ensure the prompt looks correct before plugging in real data.Podemos utilizar un prompt premade que se suele usar en este tipo de aplicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dc78c",
   "metadata": {},
   "source": [
    "Let's prepare some quesetions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab4688",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"Cuales son las cuentas que tiene que presentar una empresa?\"\n",
    "question2 = \"¿Cómo se calcula y qué significa el umbral de rentabilidad? ¿Qué decisiones estratégicas pueden derivarse de él?\"\n",
    "question3 = \"¿Qué es el EBITDA y cómo se calcula? ¿Por qué es importante para evaluar la rentabilidad de una empresa?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfd17a",
   "metadata": {},
   "source": [
    "### Retrieve the most relevant document chunks for a given question\n",
    "\n",
    "A semantic **similarity search** is performed against the Qdrant vector store using the user’s query. The top 5 most relevant chunks are returned, along with their cosine similarity scores.\n",
    "- Each result is a tuple containing a document and its corresponding score.\n",
    "- Higher scores indicate higher similarity, since cosine similarity is used.\n",
    "- The snippet prints the first 200 characters of each retrieved chunk to preview the content.\n",
    "\n",
    "This step helps validate whether the retriever is surfacing the most contextually relevant information for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e29d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = vector_store.similarity_search_with_score(question1, k=5)\n",
    "\n",
    "for doc, score in retrieved_docs:\n",
    "    print(\"Score:\", score)\n",
    "    print(\"Content:\", doc.page_content[:200], \"...\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ab8e4",
   "metadata": {},
   "source": [
    "### Format context and generate answer using the language model\n",
    "\n",
    "The **retrieved document chunks** are concatenated into a **single context string** and passed, along with the user’s question, to the prompt template. The resulting prompt is then sent to the **language model** for completion.\n",
    "- This approach ensures the model receives only the most relevant context, rather than the entire document set.\n",
    "- By combining retrieval and generation, the RAG pipeline produces answers grounded in the source material, reducing hallucinations and improving reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_content = \"\\n\\n\".join(doc.page_content for doc, _ in retrieved_docs)\n",
    "prompt_invocation = prompt.invoke({\"question\": question1, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt_invocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375fdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin-analysis-rag-8dPapl6H-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
